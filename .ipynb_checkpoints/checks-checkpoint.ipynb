{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb1ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "f = requests.get(link)\n",
    "f.encoding = f.apparent_encoding\n",
    "lines = f.text\n",
    "lines = lines.split(\n",
    "\"VENUS AND ADONIS\", 1)[1].split(\n",
    "\"*** END OF THE PROJECT GUTENBERG\", 1)[0][:500_000]\n",
    "lines = lines.lower() \n",
    "t = \" \"\n",
    "f = open('sonnets.txt', 'a')\n",
    "for sample in lines.split('\\n'):\n",
    "    if sample != '\\r':\n",
    "        if re.match('[0-9 ]+', sample) == None: # sort out the lines that only contains \n",
    "                f.write(sample)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b7ebe2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import requests\n",
    "import spacy\n",
    "import re\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "import torchdata.datapipes as dp\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad06ebf",
   "metadata": {},
   "source": [
    "see\n",
    "https://betterprogramming.pub/intro-to-rnn-character-level-text-generation-with-pytorch-db02d7e18d89\n",
    "https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aff29e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'good', 'day', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def getTokens(data_iter):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator\n",
    "    \"\"\"\n",
    "    for eng in data_iter:\n",
    "        if len(eng) != 0:\n",
    "            if re.match('[0-9 ]+', eng[0]) == None: # sort out the lines that only contains spaces and numbers\n",
    "                yield engTokenize(eng[0])\n",
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5c3ba8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'sonnets']\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = 'sonnets.txt'\n",
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='r')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\r', as_tuple=False)\n",
    "for sample in data_pipe:\n",
    "    print(engTokenize(sample[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c9bb40ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '.', ',', 'the', 'i', 'and', 'to', 'of', 'you', 'a', 'my', 'that', 'in', 'not', 'is', ';', '’s', '?', 'me', 'it', 'he', 'for', 'his', 'but', 'with', 'be', 'have']\n"
     ]
    }
   ],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "'''\n",
    "<sos> for start of sentence\n",
    "\n",
    "<eos> for end of sentence\n",
    "\n",
    "<unk> for unknown words. An example of unknown word is the one skipped because of min_freq=2.\n",
    "\n",
    "<pad> is the padding token. \n",
    "'''\n",
    "source_vocab.set_default_index(source_vocab['<unk>']) # setting <unk> instead of that unknown word\n",
    "print(source_vocab.get_itos()[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e27a3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sentence=from fairest creatures we desire increase,\n",
      "Transformed sentence=[1, 63, 1476, 1448, 51, 429, 1046, 5, 2]\n",
      "<sos> from fairest creatures we desire increase , <eos> "
     ]
    }
   ],
   "source": [
    "temp_list = list(data_pipe)\n",
    "some_sentence = temp_list[1][0]\n",
    "print(\"Some sentence=\", end=\"\")\n",
    "print(some_sentence)\n",
    "transformed_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\n",
    "print(\"Transformed sentence=\", end=\"\")\n",
    "print(transformed_sentence)\n",
    "index_to_string = source_vocab.get_itos()\n",
    "for index in transformed_sentence:\n",
    "    print(index_to_string[index], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "da5515ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from', 'fairest', 'creatures', 'we', 'desire', 'increase']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engTokenize(some_sentence)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "692afb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fairest', 'creatures', 'we', 'desire', 'increase', ',']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engTokenize(some_sentence)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d2f1ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTransform(sequence):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens and create input & target vectors\n",
    "    \"\"\"\n",
    "    tokenized = engTokenize(sequence[0])\n",
    "\n",
    "    return getTransform(source_vocab)(tokenized)\n",
    "\n",
    "        \n",
    "data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "83a481e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input sentence=<sos> the <unk> <eos> Transformed input sentence=[1, 6, 3, 2]\n",
      "\n",
      "\n",
      "Input sentence=<sos> from fairest creatures we desire increase , <eos> Transformed input sentence=[1, 63, 1476, 1448, 51, 429, 1046, 5, 2]\n",
      "\n",
      "\n",
      "Input sentence=<sos> that thereby beauty ’s rose might never die , <eos> Transformed input sentence=[1, 14, 3650, 191, 19, 818, 165, 147, 252, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sample in data_pipe:\n",
    "    print(\"\\n\\nInput sentence=\", end=\"\")\n",
    "    index_to_string = source_vocab.get_itos()\n",
    "    for index in sample:\n",
    "        print(index_to_string[index], end=\" \")\n",
    "    print(\"Transformed input sentence=\", end=\"\")\n",
    "    print(sample)\n",
    "    if i > 1:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "06b23578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 6, 3, 2], [1, 63, 1476, 1448, 51, 429, 1046, 5, 2], [1, 14, 3650, 191, 19, 818, 165, 147, 252, 5, 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 6.0000e+00, 3.0000e+00, 2.0000e+00, 1.0000e+00, 6.3000e+01,\n",
       "        1.4760e+03, 1.4480e+03, 5.1000e+01, 4.2900e+02])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pipe = list(data_pipe)\n",
    "print(data_pipe[:3])\n",
    "x = torch.Tensor([item for sublist in data_pipe for item in sublist])\n",
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b45e4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self, split: 'val' or 'train', vocab_size: int=30_000\n",
    "            ):\n",
    "        super(ShakespeareDataSet, self).__init__()\n",
    "        self.split = split\n",
    "        self.vocab_size = vocab_size\n",
    "        self.eng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\n",
    "        self.prepare_data()\n",
    "        self.configure_dataloader()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self._x[index, :]\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def engTokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize an English text and return a list of tokens\n",
    "        \"\"\"\n",
    "        return [token.text for token in self.eng.tokenizer(text)]\n",
    "\n",
    "    def getTokens(self, data_iter):\n",
    "        \"\"\"\n",
    "        Function to yield tokens from an iterator\n",
    "        \"\"\"\n",
    "        for eng in data_iter:\n",
    "            if len(eng) != 0:\n",
    "                if re.match('[0-9 ]+', eng[0]) == None: # sort out the lines that only contains spaces and numbers\n",
    "                    yield self.engTokenize(eng[0])\n",
    "    \n",
    "    def getTransform(self, vocab):\n",
    "        \"\"\"\n",
    "        Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "        of tokens.\n",
    "        \"\"\"\n",
    "        text_tranform = T.Sequential(\n",
    "            ## converts the sentences to indices based on given vocabulary\n",
    "            T.VocabTransform(vocab=vocab),\n",
    "            ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "            # 1 as seen in previous section\n",
    "            T.AddToken(1, begin=True),\n",
    "            ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "            # 2 as seen in previous section\n",
    "            T.AddToken(2, begin=False)\n",
    "        )\n",
    "        return text_tranform\n",
    "    \n",
    "    def applyTransform(sequence):\n",
    "        \"\"\"\n",
    "        Apply transforms to sequence of tokens and create input & target vectors\n",
    "        \"\"\"\n",
    "        tokenized = self.engTokenize(sequence[0])\n",
    "\n",
    "        return self.getTransform(self.source_vocab)(tokenized)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        FILE_PATH = 'sonnets.txt'\n",
    "        data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "        data_pipe = dp.iter.FileOpener(data_pipe, mode='r')\n",
    "        data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\r', as_tuple=False)\n",
    "        self.source_vocab = build_vocab_from_iterator(\n",
    "            getTokens(data_pipe),\n",
    "            min_freq=2,\n",
    "            specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "            special_first=True\n",
    "        )\n",
    "        self.source_vocab.set_default_index(source_vocab['<unk>']) # setting <unk> instead of that unknown word\n",
    "        data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\n",
    "        data_pipe = list(data_pipe)\n",
    "        self._x = torch.Tensor([item for sublist in data_pipe for item in sublist])\n",
    "        \n",
    "    def configure_dataloader(self):\n",
    "        return \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "934487bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ShakespeareDataSet at 0x7f173dde7d10>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShakespeareDataSet('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0564a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
