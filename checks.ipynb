{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ebe2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import requests\n",
    "import spacy\n",
    "import re\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "import torchdata.datapipes as dp\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad06ebf",
   "metadata": {},
   "source": [
    "see\n",
    "https://betterprogramming.pub/intro-to-rnn-character-level-text-generation-with-pytorch-db02d7e18d89\n",
    "https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb1ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "f = requests.get(link)\n",
    "f.encoding = f.apparent_encoding\n",
    "lines = f.text\n",
    "lines = lines.split(\n",
    "\"VENUS AND ADONIS\", 1)[1].split(\n",
    "\"*** END OF THE PROJECT GUTENBERG\", 1)[0][:500_000]\n",
    "lines = lines.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2da82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "t = \" \"\n",
    "f = open('sonnets.txt', 'a')\n",
    "for sample in lines.split('\\n'):\n",
    "    if sample != '\\r':\n",
    "        if re.match('[0-9 ]+', sample) == None: # sort out the lines that only contains \n",
    "                f.write(sample)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c3ba8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'sonnets']\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = 'sonnets.txt'\n",
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='r')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\r', as_tuple=False)\n",
    "for sample in data_pipe:\n",
    "    print(engTokenize(sample[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aff29e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'good', 'day', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "print(engTokenize(\"Have a good day!!!\"))\n",
    "\n",
    "def getTokens(data_iter):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator\n",
    "    \"\"\"\n",
    "    for eng in data_iter:\n",
    "        if len(eng) != 0:\n",
    "            if re.match('[0-9 ]+', eng[0]) == None: # sort out the lines that only contains spaces and numbers\n",
    "                yield engTokenize(eng[0])\n",
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9bb40ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '.', ',', 'the', 'i', 'and', 'to', 'of', 'you', 'a', 'my', 'that', 'in', 'not', 'is', ';', 'â€™s', '?', 'me', 'it', 'he', 'for', 'his', 'but', 'with', 'be', 'have']\n"
     ]
    }
   ],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "'''\n",
    "<sos> for start of sentence\n",
    "\n",
    "<eos> for end of sentence\n",
    "\n",
    "<unk> for unknown words. An example of unknown word is the one skipped because of min_freq=2.\n",
    "\n",
    "<pad> is the padding token. \n",
    "'''\n",
    "source_vocab.set_default_index(source_vocab['<unk>']) # setting <unk> instead of that unknown word\n",
    "print(source_vocab.get_itos()[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e27a3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sentence=from fairest creatures we desire increase,\n",
      "Transformed sentence=[1, 63, 1476, 1448, 51, 429, 1046, 5, 2]\n",
      "<sos> from fairest creatures we desire increase , <eos> "
     ]
    }
   ],
   "source": [
    "temp_list = list(data_pipe)\n",
    "some_sentence = temp_list[1][0]\n",
    "print(\"Some sentence=\", end=\"\")\n",
    "print(some_sentence)\n",
    "transformed_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\n",
    "print(\"Transformed sentence=\", end=\"\")\n",
    "print(transformed_sentence)\n",
    "index_to_string = source_vocab.get_itos()\n",
    "for index in transformed_sentence:\n",
    "    print(index_to_string[index], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2f1ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 63, 1476, 1448, 51, 429, 1046, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "def applyTransform(sequence):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "    return getTransform(source_vocab)(engTokenize(sequence[0]))\n",
    "        \n",
    "data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\n",
    "temp_list = list(data_pipe)\n",
    "print(temp_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6451305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
