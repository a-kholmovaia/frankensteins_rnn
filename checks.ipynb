{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b7ebe2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import requests\n",
    "import spacy\n",
    "import re\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "import torchdata.datapipes as dp\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "40b1cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train.txt'\n",
    "file=open(filename)\n",
    "lines = file.readlines()\n",
    "lines[0]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "884a1337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“I thank you, Walton,” he said, “for your kind intentions towards so miserable a wretch; but when you speak of new ties and fresh affections, think you that any can replace those who are gone? Can any man be to me as Clerval was, or any woman another Elizabeth? Even where the affections are not strongly moved by any superior excellence, the companions of our childhood always possess a certain power over our minds which hardly any later friend can obtain. They know our infantine dispositions, which, however they may be afterwards modified, are never eradicated; and they can judge of our actions with more certain conclusions as to the integrity of our motives. A sister or a brother can never, unless indeed such symptoms have been shown early, suspect the other of fraud or false dealing, when another friend, however strongly he may be attached, may, in spite of himself, be contemplated with suspicion. But I enjoyed friends, dear not only through habit and association, but from their own merits; and wherever I am, the soothing voice of my Elizabeth and the conversation of Clerval will be ever whispered in my ear. They are dead, and but one feeling in such a solitude can persuade me to preserve my life. If I were engaged in any high undertaking or design, fraught with extensive utility to my fellow creatures, then could I live to fulfil it. But such is not my destiny; I must pursue and destroy the being to whom I gave existence; then my lot on earth will be fulfilled and I may die.” '"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "55c14fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for line in lines:\n",
    "    if line[:7]=='Chapter' or line=='\\n':\n",
    "        continue\n",
    "    else:\n",
    "        output.append(line.lower())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "b21a4dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324704"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=open(filename, 'a')\n",
    "file.write(''.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad06ebf",
   "metadata": {},
   "source": [
    "see\n",
    "https://betterprogramming.pub/intro-to-rnn-character-level-text-generation-with-pytorch-db02d7e18d89\n",
    "https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "aff29e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def getTokens(data_iter):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator\n",
    "    \"\"\"\n",
    "    for eng in data_iter:\n",
    "        if len(eng) != 0:\n",
    "            if re.match('[0-9 ]+', eng[0]) == None: # sort out the lines that only contains spaces and numbers\n",
    "                yield engTokenize(eng[0])\n",
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5c3ba8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'it', 'is', 'with', 'considerable', 'difficulty', 'that', 'i', 'remember', 'the', 'original', 'era', 'of', 'my', 'being', ';', 'all', 'the', 'events', 'of', 'that', 'period', 'appear', 'confused', 'and', 'indistinct']\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = 'val.txt'\n",
    "\n",
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='r')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='.', as_tuple=True)\n",
    "for sample in data_pipe:\n",
    "    print(engTokenize(sample[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c9bb40ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', ',', 'the', 'i', '“', 'of', 'and', 'a', 'to', 'was', 'in', 'my', 'it', 'when', 'had', 'that', 'on', 'at', 'this', 'which', ';', 'as', 'but', 'by', 'day', 'found', 'with']\n"
     ]
    }
   ],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe),\n",
    "    min_freq=2,\n",
    "            specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "            special_first=True\n",
    ")\n",
    "'''\n",
    "<sos> for start of sentence\n",
    "\n",
    "<eos> for end of sentence\n",
    "\n",
    "<unk> for unknown words. An example of unknown word is the one skipped because of min_freq=2.\n",
    "\n",
    "<pad> is the padding token. \n",
    "'''\n",
    "source_vocab.set_default_index(source_vocab['<unk>']) # setting <unk> instead of that unknown word\n",
    "print(source_vocab.get_itos()[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e27a3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sentence=“it was dark when i awoke; i felt cold also, and half frightened, as it were, instinctively, finding myself so desolate\n",
      "Transformed sentence=[1, 7, 15, 12, 3, 16, 6, 43, 23, 6, 3, 66, 62, 4, 9, 3, 3, 4, 24, 15, 107, 4, 3, 4, 3, 51, 53, 3, 2]\n",
      "<sos> “ it was <unk> when i awoke ; i <unk> cold also , and <unk> <unk> , as it were , <unk> , <unk> myself so <unk> <eos> "
     ]
    }
   ],
   "source": [
    "temp_list = list(data_pipe)\n",
    "some_sentence = temp_list[1][0]\n",
    "print(\"Some sentence=\", end=\"\")\n",
    "print(some_sentence)\n",
    "transformed_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\n",
    "print(\"Transformed sentence=\", end=\"\")\n",
    "print(transformed_sentence)\n",
    "index_to_string = source_vocab.get_itos()\n",
    "for index in transformed_sentence:\n",
    "    print(index_to_string[index], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d2f1ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTransform(sequence):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens and create input & target vectors\n",
    "    \"\"\"\n",
    "    tokenized = engTokenize(sequence[0])\n",
    "    transformed = getTransform(source_vocab)(tokenized)\n",
    "\n",
    "    return (transformed[:-1], # X\n",
    "            transformed[1:]) # target\n",
    "\n",
    "\n",
    "        \n",
    "data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "83a481e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input sentence=Transformed input sentence=[1, 7, 15, 3, 29, 67, 3, 18, 6, 95, 5, 3, 3, 8, 14, 65, 23, 3, 5, 3, 8, 18, 93, 3, 3, 9, 3]\n",
      "\n",
      "\n",
      "Output sentence=“ it <unk> with considerable <unk> that i remember the <unk> <unk> of my being ; <unk> the <unk> of that period <unk> <unk> and <unk> <eos> Transformed output sentence=[7, 15, 3, 29, 67, 3, 18, 6, 95, 5, 3, 3, 8, 14, 65, 23, 3, 5, 3, 8, 18, 93, 3, 3, 9, 3, 2]\n",
      "\n",
      "\n",
      "Input sentence=Transformed input sentence=[1, 7, 15, 12, 3, 16, 6, 43, 23, 6, 3, 66, 62, 4, 9, 3, 3, 4, 24, 15, 107, 4, 3, 4, 3, 51, 53, 3]\n",
      "\n",
      "\n",
      "Output sentence=“ it was <unk> when i awoke ; i <unk> cold also , and <unk> <unk> , as it were , <unk> , <unk> myself so <unk> <eos> Transformed output sentence=[7, 15, 12, 3, 16, 6, 43, 23, 6, 3, 66, 62, 4, 9, 3, 3, 4, 24, 15, 107, 4, 3, 4, 3, 51, 53, 3, 2]\n",
      "\n",
      "\n",
      "Input sentence=Transformed input sentence=[1, 7, 101, 10, 3, 49, 3, 3, 5, 81, 9, 3, 34, 10, 3, 8, 94]\n",
      "\n",
      "\n",
      "Output sentence=“ soon a <unk> light <unk> <unk> the heavens and <unk> me a <unk> of pleasure <eos> Transformed output sentence=[7, 101, 10, 3, 49, 3, 3, 5, 81, 9, 3, 34, 10, 3, 8, 94, 2]\n",
      "\n",
      "\n",
      "Input sentence=Transformed input sentence=[1, 7, 98, 3, 8, 27, 9, 35, 92, 4, 9, 5, 3, 8, 35, 17, 48, 85, 4, 16, 6, 3, 11, 3, 14, 3, 32, 72, 3]\n",
      "\n",
      "\n",
      "Output sentence=“ several <unk> of day and night passed , and the <unk> of night had greatly lessened , when i <unk> to <unk> my <unk> from each <unk> <eos> Transformed output sentence=[7, 98, 3, 8, 27, 9, 35, 92, 4, 9, 5, 3, 8, 35, 17, 48, 85, 4, 16, 6, 3, 11, 3, 14, 3, 32, 72, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sample in data_pipe:\n",
    "    print(\"\\n\\nInput sentence=\", end=\"\")\n",
    "    index_to_string = source_vocab.get_itos()\n",
    "    #for index in sample[0]:\n",
    "        #print(index_to_string[index], end=\" \")\n",
    "    print(\"Transformed input sentence=\", end=\"\")\n",
    "    print(sample[0])\n",
    "    \n",
    "    print(\"\\n\\nOutput sentence=\", end=\"\")\n",
    "    for index in sample[1]:\n",
    "        print(index_to_string[index], end=\" \")\n",
    "    print(\"Transformed output sentence=\", end=\"\")\n",
    "    print(sample[1])\n",
    "    if i > 2:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f614e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<sos> “ it was <unk> when i awoke ; i <unk> cold also , and <unk> <unk> , as it were , <unk> , <unk> myself so <unk> \n",
      "<sos> “ several <unk> of day and night passed , and the <unk> of night had greatly lessened , when i <unk> to <unk> my <unk> from each <unk> \n",
      "<sos> “ one day , when i was <unk> by cold , i found a fire which had been <unk> by some <unk> <unk> , and was <unk> with <unk> at the warmth i experienced from it \n",
      "<sos> “ it was <unk> <unk> in the morning , and i longed to <unk> food and shelter ; at <unk> i <unk> a <unk> <unk> , on a <unk> ground , which had <unk> been <unk> for the <unk> of some <unk> \n",
      "<sos> “ it was noon when i awoke , and <unk> by the warmth of the sun , which <unk> <unk> on the <unk> ground , i <unk> to <unk> my <unk> ; and , <unk> the <unk> of the <unk> <unk> <unk> in a <unk> i found , i <unk> <unk> the <unk> for several <unk> , until at <unk> i <unk> at a <unk> 66\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sample in data_pipe:\n",
    "    if i == 0:\n",
    "        size_t = len(sample[0])\n",
    "        i += 1\n",
    "    elif size_t < len(sample[0]):\n",
    "        size_t = len(sample[0])\n",
    "        print()\n",
    "        for index in sample[0]:\n",
    "            print(index_to_string[index], end=\" \")\n",
    "print(size_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2b6f34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    " def applyPadding(pair_of_sequences):\n",
    "    \"\"\"\n",
    "    Convert sequences to tensors and apply padding\n",
    "    \"\"\"\n",
    "    return (T.PadTransform(77, 0)(T.ToTensor()(pair_of_sequences[0])), \n",
    "            T.PadTransform(77, 0)(T.ToTensor()(pair_of_sequences[1])))\n",
    "\n",
    "\n",
    "data_pipe = data_pipe.map(applyPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b45e4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataSet():\n",
    "    def __init__(\n",
    "            self, split: 'val' or 'train', vocab_size: int=30_000\n",
    "            ):\n",
    "        super(ShakespeareDataSet, self).__init__()\n",
    "        self.split = split\n",
    "        self.vocab_size = vocab_size\n",
    "        self.eng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\n",
    "        self.data_pipe = self.prepare_data()\n",
    "        self.dl = self.get_dataloader()\n",
    "    \n",
    "    def engTokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize an English text and return a list of tokens\n",
    "        \"\"\"\n",
    "        return [token.text for token in self.eng.tokenizer(text)]\n",
    "\n",
    "    def getTokens(self, data_iter):\n",
    "        \"\"\"\n",
    "        Function to yield tokens from an iterator\n",
    "        \"\"\"\n",
    "        for eng in data_iter:\n",
    "            if len(eng) != 0:\n",
    "                if re.match('[0-9 ]+', eng[0]) == None: # sort out the lines that only contains spaces and numbers\n",
    "                    yield self.engTokenize(eng[0])\n",
    "    \n",
    "    def getTransform(self, vocab):\n",
    "        \"\"\"\n",
    "        Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "        of tokens.\n",
    "        \"\"\"\n",
    "        text_tranform = T.Sequential(\n",
    "            ## converts the sentences to indices based on given vocabulary\n",
    "            T.VocabTransform(vocab=vocab),\n",
    "            ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "            # 1 as seen in previous section\n",
    "            T.AddToken(1, begin=True),\n",
    "            ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "            # 2 as seen in previous section\n",
    "            T.AddToken(2, begin=False)\n",
    "        )\n",
    "        return text_tranform\n",
    "    \n",
    "    def applyTransform(self, sequence):\n",
    "        \"\"\"\n",
    "        Apply transforms to sequence of tokens and create input & target vectors\n",
    "        \"\"\"\n",
    "        tokenized = self.engTokenize(sequence[0])\n",
    "        transformed = getTransform(self.source_vocab)(tokenized)\n",
    "\n",
    "        return (transformed[:-1], # X\n",
    "                transformed[1:]) # target\n",
    "    \n",
    "    def applyPadding(self, pair_of_sequences):\n",
    "        \"\"\"\n",
    "        Convert sequences to tensors and apply padding\n",
    "        \"\"\"\n",
    "        return (T.PadTransform(77, 0)(T.ToTensor()(list(pair_of_sequences[0]))), \n",
    "            T.PadTransform(77, 0)(T.ToTensor()(list(pair_of_sequences[1]))))\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        FILE_PATH = 'train.txt'\n",
    "        data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "        data_pipe = dp.iter.FileOpener(data_pipe, mode='r')\n",
    "        data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\r', as_tuple=False)\n",
    "        self.source_vocab = build_vocab_from_iterator(\n",
    "            self.getTokens(data_pipe),\n",
    "            min_freq=2,\n",
    "            specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "            special_first=True\n",
    "        )\n",
    "        self.source_vocab.set_default_index(self.source_vocab['<unk>']) # setting <unk> instead of that unknown word\n",
    "        self.len_vocab = len(self.source_vocab)\n",
    "        data_pipe = data_pipe.map(self.applyTransform) ## Apply the function to each element in the iterator\n",
    "        data_pipe = data_pipe.map(self.applyPadding)\n",
    "        return data_pipe\n",
    "\n",
    "    def get_dataloader(self):\n",
    "        return DataLoader(dataset=self.data_pipe, batch_size=1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "934487bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd=ShakespeareDataSet('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "bc0564a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels batch shape: torch.Size([1, 107])\n",
      "Feature batch shape: torch.Size([1, 107])\n",
      "labels = tensor([[   9,   94,   33,  514,   12,    3,    4,    7,   11,  293,   40,   48,\n",
      "            8,    5,   99, 1020,    8,   16,    3,    6,   11,    3,   17,   47,\n",
      "           29,  188,  128,    3,    7,    3,    4,    7,   11,   62,   17,  455,\n",
      "          163,  704,    3,   21,    3,    7,    3,    6,   24,   13, 1800,   33,\n",
      "           46,   54,  230,   66,   29,   31,    3,    7, 1603,  388,   10,  704,\n",
      "            3,    6,   24,  136,   31, 1992,  215, 1722,  413,   33,    5,    3,\n",
      "            8,   31,  214,   15,   12, 1956,    8,  322,   17, 1747,   31, 1653,\n",
      "          536,    4,  266,   13,   26,  209,    5, 1430,    8,   68,   16,   24,\n",
      "          130,   12,    3,    7,    5,   62,    8,   12,  293,    6,    2]])\n",
      "features = tensor([[   1,    9,   94,   33,  514,   12,    3,    4,    7,   11,  293,   40,\n",
      "           48,    8,    5,   99, 1020,    8,   16,    3,    6,   11,    3,   17,\n",
      "           47,   29,  188,  128,    3,    7,    3,    4,    7,   11,   62,   17,\n",
      "          455,  163,  704,    3,   21,    3,    7,    3,    6,   24,   13, 1800,\n",
      "           33,   46,   54,  230,   66,   29,   31,    3,    7, 1603,  388,   10,\n",
      "          704,    3,    6,   24,  136,   31, 1992,  215, 1722,  413,   33,    5,\n",
      "            3,    8,   31,  214,   15,   12, 1956,    8,  322,   17, 1747,   31,\n",
      "         1653,  536,    4,  266,   13,   26,  209,    5, 1430,    8,   68,   16,\n",
      "           24,  130,   12,    3,    7,    5,   62,    8,   12,  293,    6]])\n",
      "n_sample= 241\n"
     ]
    }
   ],
   "source": [
    "first = next(iter(sd.dl))\n",
    "labels, features = first[1], first[0]\n",
    "print(f\"Labels batch shape: {labels.size()}\")\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"{labels = }\\n{features = }\")\n",
    "n_sample = 0\n",
    "for row in iter(sd.dl):\n",
    "    n_sample += 1\n",
    "print(f\"{n_sample= }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "afbad96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_l = labels.view(-1)\n",
    "l = flat_l.shape\n",
    "arr = torch.zeros(l[0], len(sd.source_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "e64e9d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([107, 1995])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "04d09b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[range(arr.shape[0]), flat_l]= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "81087665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([107, 1995])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "6a5c77ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(arr[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "9d4541a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   9,   94,   33,  514,   12,    3,    4,    7,   11,  293,   40,   48,\n",
       "           8,    5,   99, 1020,    8,   16,    3,    6,   11,    3,   17,   47,\n",
       "          29,  188,  128,    3,    7,    3,    4,    7,   11,   62,   17,  455,\n",
       "         163,  704,    3,   21,    3,    7,    3,    6,   24,   13, 1800,   33,\n",
       "          46,   54,  230,   66,   29,   31,    3,    7, 1603,  388,   10,  704,\n",
       "           3,    6,   24,  136,   31, 1992,  215, 1722,  413,   33,    5,    3,\n",
       "           8,   31,  214,   15,   12, 1956,    8,  322,   17, 1747,   31, 1653,\n",
       "         536,    4,  266,   13,   26,  209,    5, 1430,    8,   68,   16,   24,\n",
       "         130,   12,    3,    7,    5,   62,    8,   12,  293,    6,    2])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4173fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
